SMAC era --> Social media, Mobile, Analytics & cloud

1990's --> www/dot com era, personal computers

Era of Digitization

Data Science Life cycle
-------------------------------

Data Engineering  -->  Collect, store & manage [RDBMS, Nosql db's, Hadoop, Cloud, Data Lakes, Data warehousing]

Data Analysis --> Analyse data using maths, stats & domain knowledge [Python(Pandas), R, SAS, SQL, Pyspark]

Data Visualisation --> Reports, charts, presentations [Excel, Tableau, Power BI, Qlikview, Lookr]

Machine Learning  --> Building predictive models, clusters etc. on the data [Python, Spark ML, R, Mahout]

Descriptive Analytics

Diagnostics Analytics

Predictive Analytics

Prescriptive Analytics


GIT HUB
------------

Distributed Version Control System
------------------------------------------------

Collaboration & Version Controlling
Opensource
Secure cloud storage

Git  vs Github --> Git is a desktop application, github is cloud hosted platform

Fork a repository --> Copy somebody's repo under your name
make changes
pull request

Review changes , comment & merge the request

Git commands
--------------------

Fork --> Copy somebody's repo under your name
Pull request
Clone --> Download a repo to your local machine

mkdir <folder>
ls
vi new file --> Insert for adding data, escape :wq for exiting vi editor
git add new file
git commit -m "message"
git status

git init --> The pwd is initialised as the git repository
git remote add origin "https link of the remote repo"

git pull origin master
ls
vi <file>
git add file
git commit  -m "saved"
git status

git push origin master --> This will create a pull request.

git tag --> To show the tags

upon committing a change we can specify, git tag <version>

git show <version>

Challenges with Big Data
----------------------------------
Volume --> Cost of storage, [Licensed s/w, High end servers], Portability, premature death of the death

Velocity --> Speed of data generation

Variety --> Unstructured data

Big Data Ecosystem
--------------------------
Hadoop --> Opensource, cheap commodity hardware

HDFS [Hadoop Distributed FIle System] --> Storage Layer  --> Distributed Architecture

Mapreduce 2 [YARN --> Yet Another Resource Negotiator] --> Processing Layer --> Parallel Processing

Pig --> Adhoc query language (Pig latin), data flow language , Load, Transform(generate, filter, describe, avg) & dump/store (Invented by Yahoo!)

Hive --> Data Warehousing like solution, HQL (FB)

Squoop --> Connecting with other db's

Flume --> Data integration from web

Oozie --> Work Scheduler

Zoo keeper --> Job Co-ordinator

Mahout --> ML library (Java)

HBASE --> No sql db (columnar)

Hadoop Clusters  ---> Hadoop programs running in multiple machines/servers/nodes
----------------------
Each hadoop server is capable of managing 10 TB data for ex
300*10 --> 3 PB --> 3000 TB

Modes  of cluster configuration --> Standalone, Pseudo distributed[Configured to run on single server], Fully Distributed mode[Configured to run on multiple server]

Characterstics
-------------------
Cost effective, Flexible, Scalable, Fault tolerant

Hadoop Vendors --> Apache, Cloudera, Hortonworks, MapR, MS HD Insights, IBM Big Insights

HDFS Architecture
-------------------------

Master --> Name node --> Maintains & manages the blocks which are present in the data nodes
It keeps track of entire data/ directory structure and also the placement of the blocks

Slave Machines --> Data nodes --> Data is stored & processed here. Responsible for serving the read/write requests

File storage
----------------
Files are split into physical blocks of 128 mb and further replicated 3 times by default and stored in the data nodes

Replication factor can be set by the Hadoop Admin

Hadoop 1 --> 64 mb
Hadoop 2 -->128 mb

280 mb file

The data is devided into blocks of 128 mb maximum and distributed across the data nodes in parallel.
Replications are written in serial

b1 --> 128 mb
b2 --> 128 mb
b3 --> 24 mb


Name node
----------------

Stores the meta data  --> List of files, list of blocks & its replications for each file, list of data nodes for each blocks, file attributes, access_time, transaction_log etc.

Rack Awareness --> Hadoop makes sure to store the replications in different data nodes/racks/servers.

File Read/Write Anatomy
----------------------------------

When a read/write request is submitted --> Request is taken by the Client library(Hadoop FS)
Interface between Hadoop and the User
hadoop fs  -put /source_path /destination_path

1.  Client takes the request, splits the data into blocks of 128 mb  (maximum) and replications

2. Client collects the list of data nodes details from the "Name node" to write the blocks and replications

3. Blocks are written in parallel, replications are written in serial

By default replication factor 3

Limitations of Hadoop 1
----------------------------------

Secondary Name node --> Manual back up of the name node (Single point of failure)

Single Name space --> Limited by the single ram space (Name node)

Hadoop 2
-------------

High Availability --> Active & Standby name node (Auto back-up)

Federation --> Multiple name nodes

/HR --> Name node1
/Sales --> Name node2
/Marketing --> Name node3

Commands
---------------
ls --> To list the files and directories
mkdir --> To create directory
cd --> To change the directory
gedit / vi --> i (insert), right click for paste, (esc+:wq) for quit
cat --> To view the file

Hadoop commands
---------------------------

jps --> To check the currently running daemons

HDFS  --> start-dfs.sh --> To start HDFS daemons
Mapreduce  --> start-yarn.sh --> To start mapreduce daemons
start-all.sh --> To start all the hadoop daemons
stop-all.sh --> To stop all the hadoop daemons
localhost/50070 --> To browse the hadoop file system

hadoop fs  -ls /

hadoop fs  -put      /source_path         /destination_path 

/home/hduser/ubuntu_dir   -->     hadoop fs   -put  test.txt  /hdfs_dir/

/home --> hadoop fs  -put /home/hduser/ubuntu_dir/file.txt  /hdfs_dir/new_test.txt

hadoop fs  -get /source_path /destination_path

Mapreduce / YARN [Yet Another Resource Negotiator]  --> Parallel processing framework
-------------------------------------------------------------------------

MR1 --> Job Tracker --> Master --> Burden (Managing cluster level resources, cpu/ram) scheduling of tasks, managing & monitoring every task

Task Trackers --> Slaves --> Allocation of all the resources for all the tasks

MR2/YARN
---------------
Resource manager (Master) --> Manages all the cluster level resources and scheduling of jobs

Node manager (Slave) --> Manages allocation of the jobs, one present per data node

Application master --> Present for each jobs, Negotiates with Resource Manager to schedule tasks

MR program paradigm
------------------------------

240 mb file

1b --> 128 mb
2b --> 112 mb

Split, distribute and manage  the data running in parallel with fault tolerance.

Map class --> Implements the business logic --> Overrides the map method [select * from employees where sal>10k]

Reduce class --> Aggregation logic --> Overrides the reducer method (count)

Framework itself reads the data from the blocks and gives it to the mappers

Input format class --> Responsible for reading the data from the blocks and giving it to the mappers & reducers

Text input format class --> Text files
Sequence file format --> Binary files
XML input format class --> XML files

/ * Word count program */

1. Mapper Input 

s = "Hi, we are in python class, python is awesome!"
s.split(" ")
for i in l:
     print(i,1)

hi,1
we,1
are,1
python,1
class,1
python,1
is,1
awesome,1

2. Combiner (Mini-reducer) --> (hi,1) (we,1) (are,1) (python,1,1) (class,1) (is,1) (awesome,1)

3. Reducer Input --> Same as the mapper's output

4. Reducer output --> Aggregation logic (take the sum of the values for each keys and pass it to the same reducer output)

5  Partitioner --> Responsible for sending the output of the same key to the same output directory

Print the number of emp who are getting salary > 10k in each cities

Blr --> 100
Mum --> 80
Che --> 60

Map task --> Input splits --> Num_mappers

Get data from input splits
process
produce the intermediate temporary output

Reduce task --> (By default is 1)

Reads from all the map tasks
processs
output is stored in the hdfs in blocks and replications

Print the  names of employees getting more than 10k salary? --> No aggregation is required and hence no reducer is needed

Count the number of employees getting > 10k sal? --> reducer is needed to count(aggregation)

Hadoop Streaming
--------------------------
Utility that comes along with the hadoop distribution which helps in running executable scripts of Python or R

/usr/local/hadoop-2.9.1/share/hadoop/tools/lib

chmod a+x / chmod 777 (give permission to your mapper & reducer scripts)

chmod a+x mapper.py reducer.py

#! --> Shabong --> To run a executable python script
#!/usr/bin/python

 hadoop jar /usr/local/hadoop-2.9.1/share/hadoop/tools/lib/hadoop-streaming-2.9.1.jar  -input /Pavan/myfile.txt -output /Pavan/wordcount2 -mapper /home/hduser/Pavan/mapper1.py  -reducer /home/hduser/Pavan/reducer1.py

1. Make sure the #! "Excutable script path is given"
2. verify the python syntax
3. Give permission to mapper & reducer file
4. Ensure the path of the hadoop streaming jar, input & output files, mapper & reducer files.

HIVE
-------
DWH --> OLAP
DB --> OLTP
 
Data warehousing like solution Hadoop invented FB

Lot of data

select * from data where people liked>100; 

(100 lines of code --> java)

(50 lines of code --> python)

Lot of data
Streaming data
Variety of data

With MR --> Hard to code
Solution --> They had a lot of inhouse sql developers

HQL--> Hive Query Language (Sql like)

set hive.cli.print.header=true;
set hive.cli.print.current.db=true
show databases;
create database godigit;
create database if not exists godigit;
use godigit

Internal / Managed tables  
-----------------------------------
Upon loading a file from HDFS , the file gets physically moved
from HDFS into the Hive table.

Disadvantages
--------------------
If you are deleting the table, file is permanently lost.
Data is not available for other processes of Hadoop

create table employees(name string, salary float, dept string)
row format delimited
fields terminated by ',';

// If the second line is not given (newline is assumed to be row delimiter)
//third line by default ascii is considered to be the column seperator

describe formatted database deloitte;

load data local inpath 'emp1.txt' into table employees;

describe formatted employees;

load data inpath 'emp2.txt' into table employees;  (The file from hdfs is moved into hive warehouse making it unavailable for other hadoop processes)

drop table employees (Along with table, the emp2 data is also permanently lost)

External tables
--------------------

Upon loading a file from external table, the file is not moved to the warehouse, it is available for the other processes.

# Create emp2 & emp3 and move it into hdfs to  directory "data"

create external table employee(name string, salary float, dept string)
row format delimited
fields terminated by ','
location '/data'

describe formatted employee

Partitioning
----------------
create table employee(name string, salary float, dept string)
partitioned by(state string)
row format delimited
fields terminated by ','

load data local inpath 'emp1.txt' into table employees
partition(state='MH')

load data local inpath 'emp1.txt' into table employees
partition(state='KA')

load data local inpath 'emp1.txt' into table employees
partition(state='AP')

show partitions employees

alter table employees drop if exists partition(country="India", state="KA")

create table emp(name string, salary float, dept string)
row format delimited
fields terminated by ','

load data local inpath 'emp2.txt' into table emp;

load data local inpath 'emp3.txt' overwrite into table emp;

insert overwrite directory '/aug_hdfs' select * from emp;
 
Partitioning for external tables
-------------------------------------------
create external table emp(name string, salary float, dept string)
partitioned by(country string, state string)
row format delimited
fields terminated by ','
location '/part';

alter table emp
add partition(country = "India", state="KA")
location '/part/India';

alter table emp
add partition(country = "USA", state="CA")
location '/part/India';

create table emp_kar as select name, salarym dept from employee where state='kar';
alter table emp drop if exists partition(state='kar')

alter table emp add if not exists
partition(country="Aus", State="Mb")
location '/part/aus'

alter table employees
add partition(country="US",state="TX")

show partitions
create table emp_ka as select * from employees where state="KA"
select * from emp_ka

Static & Dynamic Partitioning
---------------------------------------

load data into table static partition(dept='HR') select name,sal from master where dept='HR'

load data into table dynamic partition(dept) select name,sal,dept from master where dept='HR'

Bucketing
-------------
 create table bucket(name string, dept string, sal int)
comment 'demo of buckets'
clustered by (sal) into 2 buckets
fields terminated by ',';

Spark
=====

In-memory, open source, cluster computing framework, developed in Scala --> fast

Runs everywhere --> HDFS, Mesos, standalone cluster, data sources (AWS, s3,azure datafactory)

Unified platform --> complex programming, SQL, Dataframes, ML, streaming(real time data processing)

Master - slave architecture
=====================
Driver program (spark context)  --> Co-ordinate with the Cluster manager to allocate resources in worker nodes
worker nodes  --> Many Executors to execute tasks in parallel

Python API --> Pyspark --> Java

Python spark context --> java spark context of the JVM using --> Py4j

RDD  --> Resilient Distributed Datasets
===============================
Distributed collection of elements which are read-only(immutable), partitioned(parallelism), runs in parallel, fault tolerant

Lazy evaluation
=============

Transformations  --> The functions applied are called transformations.

DAG --> Directed Acyclic graph, upon applying a transformation Spark only creates a Logical plan of execution (estimating the computation required)

Actions  -->  Upon calling an action, the actual execution happens using the DAG reference

Persist/cache --> Spark rebuilds/re computes an RDD every time a action is called, if you need to use a RDD repeatedly, persist/cache it.

Shared Variables
----------------------
Broadcasters  --> Is to show/broadcast/joins
Accumulators --> Aggregations

Spark context
-------------------

RDD syntax --> spark context

Dataframe/sql syntax --> spark session

Spark Dataframes/Spark SQL
=======================

Create a Table for Customers and Sales data each in Hive tables using Spark SQL

Apply joins on both the tables in spark sql and save them as new tables in hive in an optimized format


Spark ML/mllib

Spark mllib --> Developed for RDD syntax (no more recommended by Apache spark)
Spark ML --> Developed for Spark DF


Spark Streaming
GraphX


Machine Learning
------------------------

Supervised ML --> Building Predictive models

Un-Supervised ML --> Clusters(Segmentations, Association rule mining)

Predictive models
-------------------------

Predictor / Independent variables

Target / Dependent variables


Regression --> Predicting continuous random variables

Classification --> Categorical/discrete variables


Linear models  --> Used when the relationship b/w target and all the x vars is easy to model (Linear Regression, Logistic regression)
Non-linear models  --> (Decision tree, SVM, KNN)
Ensemble -->  Bagging (Random forest) , Boosting (GB, ADA Boost, XG Boost)


1. Decide the target variable and check for the variables that make impact

2. Split the data randmly into training and testing

3. Choose the algorithm & build the model

4. predict on the testing dataset

5. Compare the predicted values with actual values and note the performance of the model


Linear Algorithms --> Easy to model the relationship between y & all the x variables

Non-linear Algorithms --> Complex relationship

Ensemble Algorithms  (Bagging & Boosting) --> Group of models


Linear Regression
------------------------
All x variables should have a linear relationship with the Y variable

e(y) --> b0+b1*(x1)

Logistic Regression  (Primarily designed for binary output)
-------------------------
p/(1-p) --> b0+b1*(x1)


Spark MLlib --> API on top of RDD's

Spark ML --> API for spark sql dataframes


Feature selection Techniques
---------------------------------------

P-value --> Marginal error (ho, ha)
ROCR AUC
K-best 
RFE
Chi-square

Data transformation
--------------------------
Standardisation -->  (xi-mean(x))/std(x)
Normalization --> (xi-min(x))/(max(x)-min(x))

ID                       sal                              revenue                         output?

0-2mn           20l -80l                            1b-100bn



NYSE --> 

Create a table
partitions
do joins, queries

Upload a text file as an RDD
convert it into a df
filter/select columns
create a temp view
run sql queries in pyspark
query hive table from pyspark
store data in parquet



















